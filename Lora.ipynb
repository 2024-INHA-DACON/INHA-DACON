{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kcUJKoBrermk",
    "outputId": "e6a6718b-1152-4a24-b549-81d7ced2156e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "source": "!pip install huggingface_hub",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ekdAglBUfH-j",
    "outputId": "db84f6c7-0b43-4193-a7c9-cc7c06c347a6",
    "ExecuteTime": {
     "end_time": "2024-07-04T12:04:19.956031Z",
     "start_time": "2024-07-04T12:04:18.441423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in ./.venv/lib/python3.10/site-packages (0.23.4)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (3.15.4)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (2024.5.0)\r\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (24.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (6.0.1)\r\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (4.66.4)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->huggingface_hub) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->huggingface_hub) (2.2.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.6.2)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "file_path = '/content/drive/MyDrive/DACON/INHA-DACON.jsonl'"
   ],
   "metadata": {
    "id": "Tu-64rJMfU8e"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# jsonl 파일 읽기\n",
    "data = []\n",
    "with open(file_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "# 데이터셋 객체 생성\n",
    "dataset = Dataset.from_list(data)\n",
    "\n",
    "# 모델 및 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/Llama-3-Open-Ko-8B\")\n",
    "\n",
    "# 패딩 토큰이 설정되지 않았을 경우, eos 토큰을 패딩 토큰으로 사용\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "\n",
    "# 전처리 함수\n",
    "def preprocess_function(examples):\n",
    "    inputs = [f\"{ctx} {inst}\" for ctx, inst in zip(examples['context'], examples['instruction'])]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"response\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# 매핑 적용\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "id": "XK_3gY1Lfnwl",
    "outputId": "810f2fc9-4cdc-4ed8-ae04-6804e8341a5b",
    "ExecuteTime": {
     "end_time": "2024-07-04T12:04:23.802301Z",
     "start_time": "2024-07-04T12:04:23.463559Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'HfFileSystem' from 'huggingface_hub' (/Users/sonhoyoung/Documents/Codes/Python/LLM/.venv/lib/python3.10/site-packages/huggingface_hub/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjson\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dataset, load_dataset\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoTokenizer\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# jsonl 파일 읽기\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Codes/Python/LLM/.venv/lib/python3.10/site-packages/datasets/__init__.py:17\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# See the License for the specific language governing permissions and\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[1;32m     15\u001B[0m __version__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2.20.0\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01marrow_dataset\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dataset\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01marrow_reader\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ReadInstruction\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbuilder\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001B[0;32m~/Documents/Codes/Python/LLM/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:76\u001B[0m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcontrib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconcurrent\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m thread_map\n\u001B[1;32m     75\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m config\n\u001B[0;32m---> 76\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01marrow_reader\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ArrowReader\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01marrow_writer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ArrowWriter, OptimizedTypedSequence\n\u001B[1;32m     78\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata_files\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m sanitize_patterns\n",
      "File \u001B[0;32m~/Documents/Codes/Python/LLM/.venv/lib/python3.10/site-packages/datasets/arrow_reader.py:34\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdownload\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdownload_config\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DownloadConfig\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnaming\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _split_re, filenames_for_dataset_split\n\u001B[0;32m---> 34\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtable\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m InMemoryTable, MemoryMappedTable, Table, concat_tables\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m logging\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tqdm \u001B[38;5;28;01mas\u001B[39;00m hf_tqdm\n",
      "File \u001B[0;32m~/Documents/Codes/Python/LLM/.venv/lib/python3.10/site-packages/datasets/table.py:13\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpyarrow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtypes\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m config\n\u001B[0;32m---> 13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlogging\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_logger\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m TYPE_CHECKING:\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeatures\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeatures\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Features, FeatureType\n",
      "File \u001B[0;32m~/Documents/Codes/Python/LLM/.venv/lib/python3.10/site-packages/datasets/utils/__init__.py:17\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tqdm \u001B[38;5;28;01mas\u001B[39;00m _tqdm  \u001B[38;5;66;03m# _tqdm is the module\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m experimental\n\u001B[0;32m---> 17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minfo_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m VerificationMode\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlogging\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m disable_progress_bar, enable_progress_bar, is_progress_bar_enabled\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtqdm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     20\u001B[0m     are_progress_bars_disabled,\n\u001B[1;32m     21\u001B[0m     disable_progress_bars,\n\u001B[1;32m     22\u001B[0m     enable_progress_bars,\n\u001B[1;32m     23\u001B[0m     tqdm,\n\u001B[1;32m     24\u001B[0m )\n",
      "File \u001B[0;32m~/Documents/Codes/Python/LLM/.venv/lib/python3.10/site-packages/datasets/utils/info_utils.py:8\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhuggingface_hub\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m insecure_hashlib\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m config\n\u001B[0;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexceptions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m      9\u001B[0m     ExpectedMoreDownloadedFilesError,\n\u001B[1;32m     10\u001B[0m     ExpectedMoreSplitsError,\n\u001B[1;32m     11\u001B[0m     NonMatchingChecksumError,\n\u001B[1;32m     12\u001B[0m     NonMatchingSplitsSizesError,\n\u001B[1;32m     13\u001B[0m     UnexpectedDownloadedFileError,\n\u001B[1;32m     14\u001B[0m     UnexpectedSplitsError,\n\u001B[1;32m     15\u001B[0m )\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlogging\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_logger\n\u001B[1;32m     19\u001B[0m logger \u001B[38;5;241m=\u001B[39m get_logger(\u001B[38;5;18m__name__\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/Codes/Python/LLM/.venv/lib/python3.10/site-packages/datasets/exceptions.py:5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# SPDX-License-Identifier: Apache-2.0\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# Copyright 2023 The HuggingFace Authors.\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Any, Dict, List, Optional, Union\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhuggingface_hub\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m HfFileSystem\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m config\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtable\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CastError\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'HfFileSystem' from 'huggingface_hub' (/Users/sonhoyoung/Documents/Codes/Python/LLM/.venv/lib/python3.10/site-packages/huggingface_hub/__init__.py)"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelWithHeads\n",
    "import torch\n",
    "\n",
    "model = AutoModelWithHeads.from_pretrained(\"beomi/Llama-3-Open-Ko-8B\")\n",
    "\n",
    "# 어댑터 설정\n",
    "adapter_name = model.load_adapter(\"how_to_train/your_adapter\", source=\"hf\", config=\"pfeiffer\")\n",
    "model.active_adapters = adapter_name\n",
    "\n",
    "# 어댑터 전용으로 파라미터를 훈련시키기 위해 나머지 모델 파라미터를 동결\n",
    "model.freeze_model()\n",
    "# 어댑터는 자동으로 unfreeze 됩니다.\n"
   ],
   "metadata": {
    "id": "EhM7m99ogTOs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets  # 이전에 준비한 데이터셋\n",
    ")\n",
    "\n"
   ],
   "metadata": {
    "id": "ApO7AiSdkITL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "id": "fzP7dHrlgWXo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.save_pretrained('/content/drive/MyDrive/DACON/trained_model')\n",
    "tokenizer.save_pretrained('/content/drive/MyDrive/DACON/trained_model')\n"
   ],
   "metadata": {
    "id": "S_cFa8ZngZAw"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
